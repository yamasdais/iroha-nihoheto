{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import axes3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "1. ニューラルネットワークの出力の精度を高めたい。（＝損失関数の結果を 0 に近づける）\n",
    "1. 重みのパラメータを変化させると損失関数の結果も変化する。\n",
    "1. 損失関数を重みパラメータで微分すると、損失関数が 0 に近づくような重みパラメータを探ることが出来る\n",
    "\n",
    "## 微分\n",
    "$$\n",
    "\\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x + h) - f(x)}{h}\n",
    "$$\n",
    "機械学習的には微分というよりも、ある一点（重み $x$）での傾きがわかれば良いので解析的に微分するよりは、十分に小さな $h$（大体 $10^{-4}$）程度の数値微分で代用する。\n",
    "$x+h$だけではなく $x-h$との中心を求めて（中心差分）、誤差を小さくする工夫が採られる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (h*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_1(x):\n",
    "    return 0.01 * x ** 2 + 0.1 * x\n",
    "\n",
    "def func_1_prime(x):\n",
    "    return 0.01 * 2 * x + 0.1\n",
    "\n",
    "x0 = np.arange(0.0, 10.0, 0.1)\n",
    "y0 = func_1(x0)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x0, y0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"numerical_diff: %f, prime %f\" % (numerical_diff(func_1, 5), func_1_prime(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"numerical_diff: %f, prime %f\" % (numerical_diff(func_1, 10), func_1_prime(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 偏微分\n",
    "例えば以下のような変数が複数ある($x_0, x_1$)関数があるとして、\n",
    "$$\n",
    "f(x_0, x_1) = x_0^2 + x_1^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_2(x) -> float:\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "x0, x1 = np.meshgrid(np.arange(-3, 3, 0.1), np.arange(-3, 3, 0.1))\n",
    "y1 = func_2([x0, x1])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_xlabel(\"x0\")\n",
    "ax.set_ylabel(\"x1\")\n",
    "ax.set_zlabel(\"f(x0, x1)\")\n",
    "ax.plot_wireframe(x0, x1, y1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_0, x_1$ それぞれの変数毎に微分する\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_0} = 2x_0 \\\\\n",
    "\\frac{\\partial f}{\\partial x_1} = 2x_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_2_0(x):\n",
    "    return x ** 2.0\n",
    "def func_2_prime(x):\n",
    "    return 2 * x\n",
    "\n",
    "print(\"numerical_diff: %f, prime %f\" % (numerical_diff(func_2_0, 2), func_2_prime(2)))\n",
    "print(\"numerical_diff: %f, prime %f\" % (numerical_diff(func_2_0, 3), func_2_prime(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配\n",
    "学習にあたっては偏微分をまとめて計算したいので、偏微分をまとめてベクトルとする。これを勾配(gradient)という。\n",
    "上のような関数であれば、\n",
    "$$\n",
    "\\left(\n",
    "  \\begin{array}{cc}\n",
    "    \\frac{\\partial f}{\\partial x_0} & \\frac{\\partial f}{\\partial x_1}\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "として扱う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    return np.array([numerical_diff(func_2_0, i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x0 = np.arange(-2, 2.5, 0.25)\n",
    "    x1 = np.arange(-2, 2.5, 0.25)\n",
    "    X, Y = np.meshgrid(x0, x1)\n",
    "    \n",
    "    X = X.flatten()\n",
    "    Y = Y.flatten()\n",
    "    \n",
    "    grad = numerical_gradient(func_2, np.array([X, Y]))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel('x0')\n",
    "    plt.ylabel('x1')\n",
    "    plt.grid()\n",
    "    # plt.legend()\n",
    "    plt.draw()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の関数の場合勾配の結果にマイナスを掛けると、勾配のベクトルの向きは、関数の値を最大限減らす方向を向いている。\n",
    "ただし、実際のところ勾配が向いている方向は、傾きゼロの極値の方向なので、必ずしも最小値が求まるとは限らない。\n",
    "\n",
    "この性質を利用して最適パラメータを探索（学習）する手法を勾配法(gradient method)という。\n",
    "勾配法では、勾配の方向に一定の距離だけ進む事を繰り返して、最小値を探索する。\n",
    "$$\n",
    "x_0 = x_0 - \\eta\\frac{\\partial f}{\\partial x_0} \\\\\n",
    "x_1 = x_1 - \\eta\\frac{\\partial f}{\\partial x_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\eta$ は更新量を表し学習率と呼ばれる。一回の学習でどの程度値を更新するかを定めるパラメータ。重みパラメータとは意味合いが違い、大抵は人が適当に決める。他と区別するためにハイパーパラメータと呼ばれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配降下法\n",
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    xl = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, xl)\n",
    "        xl -= lr * grad\n",
    "        \n",
    "    return xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(func_2, np.array([-3.0, 4.0]), 0.1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習率は大きすぎると発散してしまい、小さすぎると更新しきれない。だから、勾配法を使う場合には適切な学習率が設定されているかどうかをきちんと見極める必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大きすぎる lr = 10.0\n",
    "print(\"lr = 10.0: %s\" % gradient_descent(func_2, np.array([-3.0, 4.0]), 10.0, 100))\n",
    "\n",
    "# 小さすぎる lr = 1e-10\n",
    "print(\"lr = 1e-10: %s\" % gradient_descent(func_2, np.array([-3.0, 4.0]), 1e-10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
